{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will ingest and clean the NYC 311 Data and convert it to a .csv file which will later be read into the '311_data_explore.ipynb' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull 311 Service Request Data for the year 2017 from the Socrata API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in data...\n",
      "Data Download Complete!\n"
     ]
    }
   ],
   "source": [
    "offset = 0\n",
    "LIMIT = 1000000 #max rows which can be pulled from the api with a single call\n",
    "arr_of_df = [] #array which will hold multiple batches of data\n",
    "URL_CONSTANT = \"https://data.cityofnewyork.us/resource/fhrw-4uyv.json?$WHERE=created_date between '2017-01-01T00:00:00.000' and '2017-12-31T23:59:59.999' &$limit=\" + str({}) + \" &$offset=\" + str({})\n",
    "\n",
    "print(\"loading in data...\")\n",
    "\n",
    "#pull data from api in batches of 1000000 (limit) until theres no more rows to pull\n",
    "while True:\n",
    "    apiData = requests.get(URL_CONSTANT.format(LIMIT, offset)).json()\n",
    "    dfTemp = pd.DataFrame(data=apiData) #convert each batch of data into a dataframe so we can easily use the pandas concat method later\n",
    "    arr_of_df.append(dfTemp)\n",
    "    offset += LIMIT\n",
    "    if (len(apiData)-LIMIT) >= 0: #check if the max limit was pulled from the api\n",
    "        continue\n",
    "    else: #near end of dataset, no need to send another request\n",
    "        break\n",
    "        \n",
    "#concatenate all of the batches of data into one DataFrame\n",
    "df_cityData = pd.concat(arr_of_df)\n",
    "\n",
    "print(\"Data Download Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up \"Unspecified\" borough's using the dataset below (contains zipcode:borough pair):\n",
    "https://www.nycbynatives.com/nyc_info/new_york_city_zip_codes.php\n",
    "\n",
    "I copy and pasted this data into an excel spreadsheet and saved it as an csv file. There are other ways to grab the data although I found this way to be the easiest and quickest.\n",
    "\n",
    "*file already in github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean-up Complete!\n"
     ]
    }
   ],
   "source": [
    "#filter the \"Unspecified\" boroughs and their coordinating incident_zip from 'df_cityData'\n",
    "df_filteredCityData = df_cityData[['borough', 'incident_zip']].dropna()\n",
    "df_filteredCityData = df_filteredCityData.loc[df_filteredCityData['borough']=='Unspecified']\n",
    "\n",
    "df_nycZipBorough = pd.read_csv('NYC_Zips.csv') #read in the dataset specified in the markdown above\n",
    "\n",
    "#clean up the new data and format it correctly\n",
    "df_nycZipBorough = df_nycZipBorough.dropna()\n",
    "df_nycZipBorough['zipcode'] = df_nycZipBorough['zipcode'].apply(int).apply(str)#remove the '.0' from zips(.replace method was acting weird)\n",
    "df_nycZipBorough['borough'] = df_nycZipBorough['borough'].str.upper().replace('STATEN', 'STATEN ISLAND')\n",
    "df_nycZipBorough.rename(columns={'zipcode': 'incident_zip'} ,inplace=True)\n",
    "\n",
    "#merge the two dataframes (containing the unspecified boroughs and the specified ones on the 'incident_zip'),\n",
    "#dropping the \"Unspecified\" borough row in the process\n",
    "nowSpecifiedBoroughs = pd.merge(df_filteredCityData['incident_zip'].to_frame(), df_nycZipBorough, on='incident_zip').drop_duplicates()\n",
    "\n",
    "#merge the original cityData with the data containing the specified boroughs\n",
    "df_cleanedCityData = pd.merge(df_cityData, nowSpecifiedBoroughs, how='left', on='incident_zip')\n",
    "\n",
    "#Swap the \"Unspecified\" boroughs out with the \n",
    "df_cleanedCityData['borough_x']=df_cleanedCityData['borough_x'].replace('Unspecified', np.NaN)\n",
    "df_cleanedCityData.borough_x.fillna(df_cleanedCityData.borough_y, inplace=True) #fill the now 'NaN' field with the specified borough value (borough_y)\n",
    "df_cleanedCityData = df_cleanedCityData.drop(columns=['borough_y']) #drop the no longer needed row\n",
    "df_cleanedCityData.rename(columns={'borough_x': 'borough'} ,inplace=True)\n",
    "\n",
    "print(\"Clean-up Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to csv file...\n",
      "Conversion Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"converting to csv file...\")\n",
    "#convert dataframe into a csv file and save it to the current directory (should be the same as your jupyter notebook)\n",
    "df_cleanedCityData.to_csv('2017_NYC_Data.csv', index=False)\n",
    "print(\"Conversion Complete!\")\n",
    "#note: there is an opportunity to paralize the fetching step using threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
